{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_symbols = [\"AAPL\", \"GOOG\", \"MSFT\"]  \n",
    "prediction_horizon = \"daily\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Collect Data\n",
    "api_key = \"0FHEZ51MP59ZHZM0\"\n",
    "data = {}\n",
    "for stock_symbol in stock_symbols:\n",
    "    url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={stock_symbol}&apikey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    data[stock_symbol] = response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of AAPL data: 100\n",
      "Train size: 80\n",
      "Test size: 20\n",
      "Length of GOOG data: 100\n",
      "Train size: 80\n",
      "Test size: 20\n",
      "Length of MSFT data: 100\n",
      "Train size: 80\n",
      "Test size: 20\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# Preprocess the data for each stock symbol\n",
    "for stock_symbol in stock_symbols:\n",
    "    try:\n",
    "        # Transform JSON data to DataFrame\n",
    "        df = pd.DataFrame(data[stock_symbol][\"Time Series (Daily)\"]).T\n",
    "        df.columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "        df = df[['open', 'high', 'low', 'close']].astype(float)\n",
    "        df[\"date\"] = pd.to_datetime(df.index)\n",
    "        df.set_index(\"date\", inplace=True)\n",
    "        df = df.sort_index()\n",
    "        df_scaled = scaler.fit_transform(df)\n",
    "        \n",
    "        # Split data into training and testing sets\n",
    "        train_size = int(0.8 * len(df_scaled))\n",
    "        train_data.append(df_scaled[:train_size])\n",
    "        test_data.append(df_scaled[train_size:])\n",
    "    except KeyError as e:\n",
    "        print(f\"Data format error for {stock_symbol}: {e}\")\n",
    "\n",
    "    # Print the lengths of the data\n",
    "    print(f\"Length of {stock_symbol} data: {len(df_scaled)}\")\n",
    "    print(f\"Train size: {train_size}\")\n",
    "    print(f\"Test size: {len(df_scaled) - train_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data, sequence_len):\n",
    "        self.data = data\n",
    "        self.sequence_len = sequence_len\n",
    "\n",
    "        if len(self.data) <= self.sequence_len:\n",
    "            raise ValueError(f\"Data length {len(self.data)} is not sufficient for sequence length {self.sequence_len}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.data[idx:idx + self.sequence_len]\n",
    "        label = self.data[idx + self.sequence_len, 3]  # Predicting the close price\n",
    "        return {\n",
    "            'sequence': torch.tensor(sequence).float(),\n",
    "            'label': torch.tensor(label).float()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data length 30 is insufficient for sequence length 30\n",
      "Testing data length 30 is insufficient for sequence length 30\n",
      "Testing data length 30 is insufficient for sequence length 30\n"
     ]
    }
   ],
   "source": [
    "# Define sequence length and batch size\n",
    "sequence_len = 30\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoader for training and testing data\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "for stock_data in train_data:\n",
    "    if len(stock_data) > sequence_len:\n",
    "        train_datasets.append(StockDataset(stock_data, sequence_len))\n",
    "    else:\n",
    "        print(f\"Training data length {len(stock_data)} is insufficient for sequence length {sequence_len}\")\n",
    "\n",
    "for stock_data in test_data:\n",
    "    if len(stock_data) > sequence_len:\n",
    "        test_datasets.append(StockDataset(stock_data, sequence_len))\n",
    "    else:\n",
    "        print(f\"Testing data length {len(stock_data)} is insufficient for sequence length {sequence_len}\")\n",
    "\n",
    "train_dataloaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in train_datasets]\n",
    "test_dataloaders = [DataLoader(dataset, batch_size=batch_size, shuffle=False) for dataset in test_datasets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_dim).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Set device and initialize model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = LSTMModel(input_dim=4, hidden_dim=50, output_dim=1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "__len__() should return >= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m train_total_loss\u001b[38;5;241m.\u001b[39mappend(tr_epoch_loss)\n\u001b[0;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m test_dataloaders[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     31\u001b[0m         inputs_seq \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:621\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:287\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m    286\u001b[0m idx_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler:\n\u001b[0;32m    288\u001b[0m     batch[idx_in_batch] \u001b[38;5;241m=\u001b[39m idx\n\u001b[0;32m    289\u001b[0m     idx_in_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:111\u001b[0m, in \u001b[0;36mSequentialSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[1;31mValueError\u001b[0m: __len__() should return >= 0"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(range(100))\n",
    "train_total_loss = []\n",
    "test_total_loss = []\n",
    "\n",
    "for epoch in pbar:\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_dataloaders[0]:\n",
    "        optimizer.zero_grad()\n",
    "        inputs_seq = batch['sequence'].to(device)\n",
    "        targets = batch['label'].to(device)\n",
    "        tr_outputs = model(inputs_seq).squeeze()\n",
    "        tr_loss = criterion(tr_outputs, targets)\n",
    "        train_loss += tr_loss.item()\n",
    "        tr_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    tr_epoch_loss = train_loss / len(train_dataloaders[0])\n",
    "    train_total_loss.append(tr_epoch_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    for batch in test_dataloaders[0]:\n",
    "        with torch.no_grad():\n",
    "            inputs_seq = batch['sequence'].to(device)\n",
    "            targets = batch['label'].to(device)\n",
    "            te_outputs = model(inputs_seq).squeeze()\n",
    "            te_loss = criterion(te_outputs, targets)\n",
    "            test_loss += te_loss.item()\n",
    "    \n",
    "    te_epoch_loss = test_loss / len(test_dataloaders[0])\n",
    "    test_total_loss.append(te_epoch_loss)\n",
    "    pbar.set_description(f\"Epoch {epoch+1}, Train Loss: {tr_epoch_loss:.4f}, Test Loss: {te_epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Fine-Tuning\n",
    "def fine_tune(model, stock_symbols, data, sequence_len, batch_size, epochs):\n",
    "    best_model = model\n",
    "    best_loss = float('inf')\n",
    "    for stock_symbol in stock_symbols:\n",
    "        X_train, X_test, y_train, y_test = split_data(data[stock_symbol], sequence_len)\n",
    "        train_dataloader = DataLoader(dataset=StockDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "        test_dataloader = DataLoader(dataset=StockDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for i, (inputs, targets) in enumerate(train_dataloader):\n",
    "                optimizer.zero_grad()\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.squeeze().to(device)\n",
    "                outputs = model(inputs).squeeze()\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                total_test_loss = 0\n",
    "                for i, (inputs, targets) in enumerate(test_dataloader):\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.squeeze().to(device)\n",
    "                    outputs = model(inputs).squeeze()\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    total_test_loss += loss.item()\n",
    "            test_loss = total_test_loss / len(test_dataloader)\n",
    "            if test_loss < best_loss:\n",
    "                best_loss = test_loss\n",
    "                best_model = model\n",
    "            print(f\"Epoch {epoch+1}, Stock {stock_symbol}, Train Loss: {total_loss / len(train_dataloader):.4f}, Test Loss: {test_loss:.4f}\")\n",
    "    return best_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
